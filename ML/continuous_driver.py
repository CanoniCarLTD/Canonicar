import os
import sys
import time
import random
import numpy as np
import argparse
import logging
import pickle
import torch
from distutils.util import strtobool
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

# from encoder_init import EncodeState
from ML.networks.agent import PPOAgent
from simulation.connection import ClientConnection
from simulation.environment import CarlaEnvironment
from parameters import *


class ContinuousDriver:
    def __init__(self):
        self.args = self.parse_args()
        self.device = self.setDevice()
        self.deterministic_cuda()

        # ========================================================================
        #                      BASIC PARAMETER & LOGGING SETUP
        # ========================================================================

        self.exp_name = self.args.exp_name
        self.run_name = "PPO"
        self.train: bool = self.args.train
        self.town: str = self.args.town
        self.checkpoint_load: bool = self.args.load_checkpoint
        self.total_timesteps: int = self.args.total_timesteps

        # ========================================================================
        #            PREPARING THE PARAMETERS FOR THE ALGORITHM
        # ========================================================================

        self.action_std_init: float = self.args.action_std_init
        self.action_std_decay_rate: float = 0.05
        self.min_action_std: float = 0.05
        self.action_std_decay_freq: int = 5e5
        self.timestep: int = 0
        self.episode: int = 0
        self.cumulative_score: int = 0
        self.episodic_length: list = list()
        self.scores: list = list()
        self.deviation_from_center: int = 0
        self.distance_covered: int = 0

    # ========================================================================
    #                                 SET DEVICE
    # ========================================================================

    def setDevice(self):
        print(
            "============================================================================================"
        )
        # set device to cpu or cuda
        self.device = torch.device("cpu")
        if torch.cuda.is_available():
            self.device = torch.device("cuda:0")
            torch.cuda.empty_cache()
            print("Using CUDA")
            print("Device set to : " + str(torch.cuda.get_device_name(self.device)))
        else:
            print("Device set to : cpu")
        print(
            "============================================================================================"
        )

    def parse_args(self):
        parser = argparse.ArgumentParser()

        parser.add_argument("--exp-name", type=str, help="name of the experiment")

        parser.add_argument(
            "--env-name",
            type=str,
            default="carla",
            help="name of the simulation environment",
        )

        parser.add_argument(
            "--learning-rate",
            type=float,
            default=PPO_LEARNING_RATE,
            help="learning rate of the optimizer",
        )

        """
        By setting a seed, we ensure that the sequence of random numbers generated by the pseudorandom number generator (PRNG) is reproducible. This is useful cause we want to ensure that your results can be replicated exactly.
        """
        parser.add_argument(
            "--seed", type=int, default=SEED, help="seed of the experiment"
        )

        """
        Timestep is a discrete point in time where the state of the system is updated based on the model's actions and the environment's response.
        """
        parser.add_argument(
            "--total-timesteps",
            type=int,
            default=TOTAL_TIMESTEPS,
            help="total timesteps of the experiment",
        )

        parser.add_argument(
            "--action-std-init",
            type=float,
            default=ACTION_STD_INIT,
            help="initial exploration noise",
        )

        parser.add_argument(
            "--test-timesteps",
            type=int,
            default=TEST_TIMESTEPS,
            help="timesteps to test our model",
        )

        """
        The episode length specifies the maximum number of timesteps that an episode can have. If the terminal state is not reached within this number of timesteps, the episode will be forcibly terminated. The terminal state could be a predefined condition such as reaching a goal, running out of time, or encountering a failure state.
        """
        parser.add_argument(
            "--episode-length",
            type=int,
            default=EPISODE_LENGTH,
            help="max timesteps in an episode",
        )

        parser.add_argument(
            "--train", default=True, action="store_true", help="is it training?"
        )

        parser.add_argument(
            "--town", type=str, default="Town07", help="which town do we load?"
        )

        # To replace the above line with the following line
        parser.add_argument(
            "--track", type=str, default="Town07", help="which track are we on?"
        )

        parser.add_argument(
            "--load-checkpoint", type=bool, default=MODEL_LOAD, help="resume training?"
        )

        """
        If True, forces PyTorch (cuDNN) to only use deterministic convolution algorithms, ensuring reproducibility.
        """
        parser.add_argument(
            "--torch-deterministic",
            type=lambda x: bool(strtobool(x)),
            default=True,
            nargs="?",
            const=True,
            help="if True, `torch.backends.cudnn.deterministic=True`",
        )

        parser.add_argument(
            "--cuda",
            type=lambda x: bool(strtobool(x)),
            nargs="?",
            const=True,  # If --cuda is used without a value, set it to True
            default=torch.cuda.is_available(),  # Default to CUDA if available, otherwise False
            help="Enable CUDA if available (default: auto-detect)",
        )

        args = parser.parse_args()

        return args

    # ========================================================================
    #            ENSURE DETERMINISTIC SETTINGS BASED ON USER INPUT
    # ========================================================================

    def deterministic_cuda(self):
        if torch.cuda.is_available():
            # Ensure CuDNN is enabled and set deterministic mode based on the user's preference
            if torch.backends.cudnn.enabled:
                print("CuDNN is enabled.")
                if self.args.torch_deterministic:
                    print("Setting CuDNN to deterministic mode.")
                    torch.backends.cudnn.deterministic = True
                    torch.backends.cudnn.benchmark = (
                        False  # Disable auto-tuner for deterministic behavior
                    )
                else:
                    torch.backends.cudnn.deterministic = False
                    torch.backends.cudnn.benchmark = (
                        True  # Enable auto-tuner for faster performance
                    )
            else:
                print(
                    "Warning: CuDNN is not enabled. Training may be significantly slower."
                )

    def boolean_string(self, s):
        if s not in {"False", "True"}:
            raise ValueError("Not a valid boolean string")
        return s == "True"

    # ========================================================================
    #        A HELPER FUNCTION FOR LOGGING TRAINING AND TESTING METRICS
    # ========================================================================

    def log_training_metrics(self):
        """Logs the training metrics to tensorboard
        Logs: episodic reward/episode, cumulative reward/info, cumulative reward/(t), average episodic reward/info, average reward/(t), episode length (s)/info, reward/(t), average deviation from center/episode, average deviation from center/(t), average distance covered (m)/episode, average distance covered (m)/(t)
        """
        self.writer.add_scalar("Episodic Reward/episode", self.scores[-1], self.episode)
        self.writer.add_scalar(
            "Cumulative Reward/info", self.cumulative_score, self.episode
        )
        self.writer.add_scalar(
            "Cumulative Reward/(t)", self.cumulative_score, self.timestep
        )
        self.writer.add_scalar(
            "Average Episodic Reward/info", np.mean(self.scores[-5]), self.episode
        )
        self.writer.add_scalar(
            "Average Reward/(t)", np.mean(self.scores[-5]), self.timestep
        )
        self.writer.add_scalar(
            "Episode Length (s)/info", np.mean(self.episodic_length), self.episode
        )
        self.writer.add_scalar("Reward/(t)", self.scores[-1], self.timestep)
        self.writer.add_scalar(
            "Average Deviation from Center/episode",
            self.deviation_from_center / 5,
            self.episode,
        )
        self.writer.add_scalar(
            "Average Deviation from Center/(t)",
            self.deviation_from_center / 5,
            self.timestep,
        )
        self.writer.add_scalar(
            "Average Distance Covered (m)/episode",
            self.distance_covered / 5,
            self.episode,
        )
        self.writer.add_scalar(
            "Average Distance Covered (m)/(t)", self.distance_covered / 5, self.timestep
        )

    def log_testing_metrics(self):
        """Logs the testing metrics to tensorboard
        Logs: episodic reward, cumulative reward/info, cumulative reward/(t), episode lengths (s), reward/(t), deviation from center, distance covered (m)/episode, distance covered (m)/(t)
        """
        self.writer.add_scalar(
            "TEST: Episodic Reward/episode", self.scores[-1], self.episode
        )
        self.writer.add_scalar(
            "TEST: Cumulative Reward/info", self.cumulative_score, self.episode
        )
        self.writer.add_scalar(
            "TEST: Cumulative Reward/(t)", self.cumulative_score, self.timestep
        )
        self.writer.add_scalar(
            "TEST: Episode Length (s)/info", np.mean(self.episodic_length), self.episode
        )
        self.writer.add_scalar("TEST: Reward/(t)", self.scores[-1], self.timestep)
        self.writer.add_scalar(
            "TEST: Deviation from Center/episode",
            self.deviation_from_center,
            self.episode,
        )
        self.writer.add_scalar(
            "TEST: Deviation from Center/(t)", self.deviation_from_center, self.timestep
        )
        self.writer.add_scalar(
            "TEST: Distance Covered (m)/episode", self.distance_covered, self.episode
        )
        self.writer.add_scalar(
            "TEST: Distance Covered (m)/(t)", self.distance_covered, self.timestep
        )

    def runner(self):
        if self.train == True:
            writer = SummaryWriter(
                f"runs/{self.run_name}_{self.action_std_init}_{int(self.total_timesteps)}/{self.town}"
            )
        else:
            writer = SummaryWriter(
                f"runs/{self.run_name}_{self.action_std_init}_{int(self.total_timesteps)}_TEST/{self.town}"
            )
        writer.add_text(
            "hyperparameters",
            "|param|value|\n|-|-|\n%s"
            % (
                "\n".join([f"|{key}|{value}" for key, value in vars(self.args).items()])
            ),
        )

        # Seeding to reproduce the results
        random.seed(self.args.seed)
        np.random.seed(self.args.seed)
        torch.manual_seed(self.args.seed)

        self.createEnvironment()

        try:
            time.sleep(0.5)

            if self.checkpoint_load:
                chkt_file_nums = (
                    len(next(os.walk(f"checkpoints/PPO/{self.town}"))[2]) - 1
                )
                chkpt_file = (
                    f"checkpoints/PPO/{self.town}/checkpoint_ppo_"
                    + str(chkt_file_nums)
                    + ".pickle"
                )
                with open(chkpt_file, "rb") as f:
                    data = pickle.load(f)
                    self.episode = data["episode"]
                    self.timestep = data["timestep"]
                    self.cumulative_score = data["cumulative_score"]
                    self.action_std_init = data["action_std_init"]
                self.agent = PPOAgent(self.town, self.action_std_init)
                self.agent.chkpt_load()
            else:
                if self.train == False:
                    self.agent = PPOAgent(self.town, self.action_std_init)
                    self.agent.chkpt_load()
                    for params in self.agent.old_policy.actor.parameters():
                        params.requires_grad = False
                else:
                    self.agent = PPOAgent(self.town, self.action_std_init)

            if self.train:
                self.training()
            else:
                self.testing()

        finally:
            sys.exit()

    # ========================================================================
    #                           CREATING THE ENIVRONMENT
    # ========================================================================

    def createEnvironment(self):
        self.clientConnection = ClientConnection("Town10HD")
        self.client, self.world = self.clientConnection.setup()
        # Pull from future connection implementation

        if self.train:
            self.env = CarlaEnvironment(self.client, self.world, self.town)
            print("Training Environment Created")
        else:
            self.env = CarlaEnvironment(
                self.client, self.world, self.town, checkpoint_frequency=None
            )
            print("Testing Environment Created")

    def training(self):
        # Training
        while self.timestep < self.total_timesteps:
            observation = self.env.reset()
            print("Observation recieved after env.reset()")
            """ THIS NOW NEEDS TO GO TO OUR CONVOLUTION ! """
            """
            observation = encode.process(observation)
            """
            # observation= convolution(observation) #IDEA (ETAI)
            self.current_ep_reward = 0
            t1 = datetime.now()
            for t in range(self.args.episode_length):
                # Select action with policy
                print("going to get action()")
                action = self.agent.get_action(observation, train=True)
                print("action recieved: ", action)
                # Take step in the environment
                observation, reward, done, info = self.env.step(action)
                if observation is None:
                    break
                """ THIS AGAIN NEEDS TO GO TO OUR CONVOLUTION ! """
                """
                observation = encode.process(observation)
                """
                # observation= convolution(observation) #IDEA (ETAI)
                self.agent.memory.rewards.append(reward)
                self.agent.memory.dones.append(done)
                self.timestep += 1
                self.current_ep_reward += reward
                # decay action standard deviation
                if self.timestep % self.action_std_decay_freq == 0:
                    self.action_std_init = self.agent.decay_action_std(
                        self.action_std_decay_rate, self.min_action_std
                    )
                if self.timestep == self.total_timesteps - 1:
                    self.agent.chkpt_save()
                # break; if the episode is over
                if done:
                    self.episode += 1
                    t2 = datetime.now()
                    t3 = t2 - t1
                    self.episodic_length.append(abs(t3.total_seconds()))
                    break
            self.distance_covered += info[0]
            self.deviation_from_center += info[1]
            self.scores.append(self.current_ep_reward)
            if self.checkpoint_load:
                # Update the cumulative score by adding the current episode's reward to the previous average
                self.cumulative_score = (
                    (self.cumulative_score * (self.episode - 1))
                    + self.current_ep_reward
                ) / (self.episode)
            else:
                self.cumulative_score = np.mean(self.scores)
            print(
                "Episode: {}".format(self.episode),
                ", Timestep: {}".format(self.timestep),
                ", Reward:  {:.2f}".format(self.current_ep_reward),
                ", Average Reward:  {:.2f}".format(self.cumulative_score),
            )
            if self.episode % 10 == 0:
                self.agent.learn()
                self.agent.chkpt_save()
                chkt_file_nums = len(next(os.walk(f"checkpoints/PPO/{self.town}"))[2])
                if chkt_file_nums != 0:
                    chkt_file_nums -= 1
                chkpt_file = (
                    f"checkpoints/PPO/{self.town}/checkpoint_ppo_"
                    + str(chkt_file_nums)
                    + ".pickle"
                )
                data_obj = {
                    "cumulative_score": self.cumulative_score,
                    "episode": self.episode,
                    "timestep": self.timestep,
                    "action_std_init": self.action_std_init,
                }
                with open(chkpt_file, "wb") as handle:
                    pickle.dump(data_obj, handle)
            if self.episode % 5 == 0:
                self.log_training_metrics()
                self.episodic_length = list()
                self.deviation_from_center = 0
                self.distance_covered = 0
            if self.episode % 100 == 0:
                self.agent.save()
                self.chkt_file_nums = len(
                    next(os.walk(f"checkpoints/PPO/{self.town}"))[2]
                )
                chkpt_file = (
                    f"checkpoints/PPO/{self.town}/checkpoint_ppo_"
                    + str(chkt_file_nums)
                    + ".pickle"
                )
                data_obj = {
                    "cumulative_score": self.cumulative_score,
                    "episode": self.episode,
                    "timestep": self.timestep,
                    "action_std_init": self.action_std_init,
                }
                with open(chkpt_file, "wb") as handle:
                    pickle.dump(data_obj, handle)
        print("Terminating the run.")
        sys.exit()

    def testing(self):
        # Testing
        while self.timestep < self.args.test_timesteps:
            observation = self.env.reset()
            """
            observation = encode.process(observation)
            """
            self.current_ep_reward = 0
            t1 = datetime.now()
            for t in range(self.args.episode_length):
                # select action with policy
                action = self.agent.get_action(observation, train=False)
                observation, reward, done, info = self.env.step(action)
                if observation is None:
                    break
                """
                observation = encode.process(observation)
                """
                self.timestep += 1
                self.current_ep_reward += reward
                # break; if the episode is over
                if done:
                    self.episode += 1
                    t2 = datetime.now()
                    t3 = t2 - t1
                    self.episodic_length.append(abs(t3.total_seconds()))
                    break
            self.deviation_from_center += info[1]
            self.distance_covered += info[0]
            self.scores.append(self.current_ep_reward)
            self.cumulative_score = np.mean(self.scores)
            print(
                "Episode: {}".format(self.episode),
                ", Timestep: {}".format(self.timestep),
                ", Reward:  {:.2f}".format(self.current_ep_reward),
                ", Average Reward:  {:.2f}".format(self.cumulative_score),
            )
            self.log_testing_metrics()
            self.episodic_length = list()
            self.deviation_from_center = 0
            self.distance_covered = 0
        print("Terminating the run.")
        sys.exit()


def runner():

    # ========================================================================
    #        A HELPER FUNCTION FOR LOGGING TRAINING AND TESTING METRICS
    # ========================================================================

    # def log_training_metrics():
    #     """Logs the training metrics to tensorboard
    #     Logs: episodic reward/episode, cumulative reward/info, cumulative reward/(t), average episodic reward/info, average reward/(t), episode length (s)/info, reward/(t), average deviation from center/episode, average deviation from center/(t), average distance covered (m)/episode, average distance covered (m)/(t)
    #     """
    #     writer.add_scalar("Episodic Reward/episode", scores[-1], episode)
    #     writer.add_scalar("Cumulative Reward/info", cumulative_score, episode)
    #     writer.add_scalar("Cumulative Reward/(t)", cumulative_score, timestep)
    #     writer.add_scalar("Average Episodic Reward/info", np.mean(scores[-5]), episode)
    #     writer.add_scalar("Average Reward/(t)", np.mean(scores[-5]), timestep)
    #     writer.add_scalar("Episode Length (s)/info", np.mean(episodic_length), episode)
    #     writer.add_scalar("Reward/(t)", scores[-1], timestep)
    #     writer.add_scalar(
    #         "Average Deviation from Center/episode", deviation_from_center / 5, episode
    #     )
    #     writer.add_scalar(
    #         "Average Deviation from Center/(t)", deviation_from_center / 5, timestep
    #     )
    #     writer.add_scalar(
    #         "Average Distance Covered (m)/episode", distance_covered / 5, episode
    #     )
    #     writer.add_scalar(
    #         "Average Distance Covered (m)/(t)", distance_covered / 5, timestep
    #     )

    # def log_testing_metrics():
    #     """Logs the testing metrics to tensorboard
    #     Logs: episodic reward, cumulative reward/info, cumulative reward/(t), episode lengths (s), reward/(t), deviation from center, distance covered (m)/episode, distance covered (m)/(t)
    #     """
    #     writer.add_scalar("TEST: Episodic Reward/episode", scores[-1], episode)
    #     writer.add_scalar("TEST: Cumulative Reward/info", cumulative_score, episode)
    #     writer.add_scalar("TEST: Cumulative Reward/(t)", cumulative_score, timestep)
    #     writer.add_scalar(
    #         "TEST: Episode Length (s)/info", np.mean(episodic_length), episode
    #     )
    #     writer.add_scalar("TEST: Reward/(t)", scores[-1], timestep)
    #     writer.add_scalar(
    #         "TEST: Deviation from Center/episode", deviation_from_center, episode
    #     )
    #     writer.add_scalar(
    #         "TEST: Deviation from Center/(t)", deviation_from_center, timestep
    #     )
    #     writer.add_scalar(
    #         "TEST: Distance Covered (m)/episode", distance_covered, episode
    #     )
    #     writer.add_scalar("TEST: Distance Covered (m)/(t)", distance_covered, timestep)

    # args = parse_args()
    # exp_name: str = args.exp_name
    # run_name = "PPO"
    # train: bool = args.train
    # town: str = args.town
    # checkpoint_load: bool = args.load_checkpoint
    # total_timesteps: int = args.total_timesteps
    # action_std_init: float = args.action_std_init

    # if torch.cuda.is_available():
    #     # Ensure CuDNN is enabled and set deterministic mode based on the user's preference
    #     if torch.backends.cudnn.enabled:
    #         print("CuDNN is enabled.")
    #         if args.torch_deterministic:
    #             print("Setting CuDNN to deterministic mode.")
    #             torch.backends.cudnn.deterministic = True
    #             torch.backends.cudnn.benchmark = (
    #                 False  # Disable auto-tuner for deterministic behavior
    #             )
    #         else:
    #             torch.backends.cudnn.deterministic = False
    #             torch.backends.cudnn.benchmark = (
    #                 True  # Enable auto-tuner for faster performance
    #             )
    #     else:
    #         print(
    #             "Warning: CuDNN is not enabled. Training may be significantly slower."
    #         )

    # ========================================================================
    #            PREPARING THE PARAMETERS FOR THE ALGORITHM
    # ========================================================================

    # action_std_decay_rate = 0.05
    # min_action_std = 0.05
    # action_std_decay_freq = 5e5  # 500,000
    # timestep = 0
    # episode = 0
    # cumulative_score = 0
    # episodic_length = list()
    # scores = list()
    # deviation_from_center = 0
    # distance_covered = 0

    # # ========================================================================
    # #                           CREATING THE ENIVRONMENT
    # # ========================================================================

    # clientConnection = ClientConnection("Town10HD")
    # client, world = clientConnection.setup()
    # # Pull from future connection implementation

    # if train:
    #     env = CarlaEnvironment(client, world, town)
    #     print("Training Environment Created")
    # else:
    #     env = CarlaEnvironment(client, world, town, checkpoint_frequency=None)
    #     print("Testing Environment Created")

    """
    encode = EncodeState(LATENT_DIM)
    """
    # ========================================================================
    #                           ALGORITHM
    # ========================================================================
    # try:
    #     time.sleep(0.5)

    #     if checkpoint_load:
    #         chkt_file_nums = len(next(os.walk(f"checkpoints/PPO/{town}"))[2]) - 1
    #         chkpt_file = (
    #             f"checkpoints/PPO/{town}/checkpoint_ppo_"
    #             + str(chkt_file_nums)
    #             + ".pickle"
    #         )
    #         with open(chkpt_file, "rb") as f:
    #             data = pickle.load(f)
    #             episode = data["episode"]
    #             timestep = data["timestep"]
    #             cumulative_score = data["cumulative_score"]
    #             action_std_init = data["action_std_init"]
    #         agent = PPOAgent(town, action_std_init)
    #         agent.chkpt_load()
    #     else:
    #         if train == False:
    #             agent = PPOAgent(town, action_std_init)
    #             agent.chkpt_load()
    #             for params in agent.old_policy.actor.parameters():
    #                 params.requires_grad = False
    #         else:
    #             agent = PPOAgent(town, action_std_init)

    #     if train:
    #         training()
    #     else:
    #         testing()

    # finally:
    #     sys.exit()


if __name__ == "__main__":
    try:
        continuous_Driver = ContinuousDriver()
        continuous_Driver.runner()

    except KeyboardInterrupt:
        sys.exit()
    finally:
        print("\nExit")
